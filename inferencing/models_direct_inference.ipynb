{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sentiment analysis with probabilities saved to 'sentiment_data_news_2yr.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check for CUDA\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"âœ… Using CUDA\" if device == 0 else \"âš ï¸ CUDA not available, using CPU\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"stock_data.csv\")\n",
    "df = df.dropna(subset=[\"summary\", \"headline\"], how=\"all\")\n",
    "df[\"summary\"] = df[\"summary\"].fillna(df[\"headline\"])\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "finbert = BertForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\", num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "finbert.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    finbert.cuda()\n",
    "\n",
    "# Text prompts\n",
    "prompts = [\n",
    "    f\"Financial Sentiment Analysis for {ticker}:\\nNews: \\\"{text}\\\"\\nDetermine if this news is financially positive, neutral, or negative.\"\n",
    "    for text, ticker in zip(df[\"summary\"], df[\"Ticker\"])\n",
    "]\n",
    "\n",
    "# Tokenize in batches\n",
    "batch_size = 32\n",
    "all_probs = []\n",
    "\n",
    "for i in range(0, len(prompts), batch_size):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    if device == 0:\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = finbert(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    all_probs.extend(probs)\n",
    "\n",
    "# Add sentiment probability columns\n",
    "df[\"FinBERT_neutral\"] = [float(p[0]) for p in all_probs]\n",
    "df[\"FinBERT_positive\"] = [float(p[1]) for p in all_probs]\n",
    "df[\"FinBERT_negative\"] = [float(p[2]) for p in all_probs]\n",
    "\n",
    "# Save the result with probability columns only\n",
    "df.to_csv(\"stock_data.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Sentiment analysis with probabilities saved to 'sentiment_data_news_2yr.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Closing Price for 2025-02-01: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "# === 1. Define the GEN AI (Transformer) Model ===\n",
    "class StockGenModel(nn.Module):\n",
    "    def __init__(self, feature_dim=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(feature_dim, 64)  # Embed each timestep\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=64, nhead=4),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 1)  # Predict 1 value (next day's close)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch, seq_len, feature_dim) -> (batch, seq_len, 64)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# === 2. Load CSV ===\n",
    "csv_path = \"sentiment_analysis_aapl.csv\"  # <<-- your CSV file path\n",
    "feature_columns = [\"Close_t-1\", \"Close_t-2\", \"Close_t-3\", \"Close_t-4\", \n",
    "                   \"Close_t-5\", \"Close_t-6\", \"Close_t-7\", \"Sentiment\"]\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === 3. Map Sentiment Strings to Numbers ===\n",
    "sentiment_mapping = {\n",
    "    'positive': 2,\n",
    "    'neutral': 1,\n",
    "    'negative': 0\n",
    "}\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# === 4. Preprocess Features ===\n",
    "features = df[feature_columns].values\n",
    "features = torch.tensor(features, dtype=torch.float32)  # (seq_len, feature_dim)\n",
    "features = features.unsqueeze(0)  # add batch dimension => (1, seq_len, feature_dim)\n",
    "\n",
    "# === 5. Create Model and Predict ===\n",
    "model = StockGenModel(feature_dim=features.shape[-1])\n",
    "model.eval()  # inference mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(features)\n",
    "    prediction_value = prediction.item()\n",
    "\n",
    "# === 6. Print Prediction with Date ===\n",
    "last_date = pd.to_datetime(df['Date'].values[-1])\n",
    "predicted_date = last_date + pd.Timedelta(days=1)\n",
    "\n",
    "print(f\"Predicted Closing Price for {predicted_date.date()}: {prediction_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Trend: Uptrend\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Smaller model: Flan-T5 Small\n",
    "base_model = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Prepare the input\n",
    "previous_prices = df[[\"Close_t-1\", \"Close_t-2\", \"Close_t-3\",\n",
    "                      \"Close_t-4\", \"Close_t-5\", \"Close_t-6\", \"Close_t-7\"]].iloc[-1].tolist()\n",
    "\n",
    "trend_input = f\"\"\"\n",
    "You are a financial expert. Based on stock prices and a predicted value, classify the trend as Uptrend, Downtrend or Sideways.\n",
    "\n",
    "Previous 7 closing prices: {previous_prices}\n",
    "Predicted next price: {prediction_value:.2f}\n",
    "\n",
    "Classify the stock trend strictly as one of these: Downtrend, Uptrend, Sideways.\n",
    "\n",
    "Answer in exactly ONE WORD ONLY.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(trend_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():  # prevent gradient tracking, saves memory\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=3,\n",
    "        temperature=0.1,\n",
    "        do_sample=False,             # deterministic output\n",
    "        use_cache=True               # faster generation\n",
    "    )\n",
    "\n",
    "trend = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Predicted Trend: {trend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load Flan-T5 Base\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "flan_model.eval()\n",
    "\n",
    "# Prepare past prices\n",
    "past_prices = df[[\"Close_t-1\", \"Close_t-2\", \"Close_t-3\",\n",
    "                  \"Close_t-4\", \"Close_t-5\", \"Close_t-6\", \"Close_t-7\"]].iloc[-1].tolist()\n",
    "sentiment_label = df[\"Sentiment\"].iloc[-1]\n",
    "\n",
    "# Optional: Convert numeric sentiment back to string\n",
    "inv_sentiment_map = {v: k for k, v in sentiment_mapping.items()}\n",
    "sentiment_str = inv_sentiment_map.get(sentiment_label, \"neutral\")\n",
    "\n",
    "# Prepare input prompt (more explicit and detailed)\n",
    "confidence_input = (\n",
    "    f\"As a financial expert, you are tasked with predicting the confidence level of a stock trend continuing. \"\n",
    "    f\"Here are the details: \\n\"\n",
    "    f\"Trend: {trend}\\n\"\n",
    "    f\"Sentiment: {sentiment_str}\\n\"\n",
    "    f\"Past 7-Day Closing Prices: {past_prices}\\n\"\n",
    "    f\"Please output a confidence score as a percentage (0-100%). \"\n",
    "    f\"Respond only with a number followed by '%', e.g., '75%'\"\n",
    ")\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(confidence_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = flan_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,  # Allow more space for model to generate\n",
    "        temperature=0.5,  # Experiment with slightly higher temperature for diversity\n",
    "        top_p=0.9,  # Try top_p sampling for more structured output\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "\n",
    "# Extract confidence value (check if the match is better)\n",
    "confidence_match = re.search(r\"(\\d{1,3}(\\.\\d+)?)\", generated_text)  # Handle decimal percentages\n",
    "\n",
    "if confidence_match:\n",
    "    confidence_value = float(confidence_match.group(1))  # Convert string to float\n",
    "    confidence_percentage = confidence_value * 100  # Convert to percentage\n",
    "    confidence = f\"{confidence_percentage:.2f}\"  # Format as a percentage with two decimals\n",
    "else:\n",
    "    confidence = \"0\"  # Default to 0% if no match is found\n",
    "\n",
    "print(f\"Confidence Score: {confidence}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volatility Estimate (std dev): 6.9671\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# === Extract last 7 days of closing prices ===\n",
    "past_prices = df[[\"Close_t-1\", \"Close_t-2\", \"Close_t-3\",\n",
    "                  \"Close_t-4\", \"Close_t-5\", \"Close_t-6\", \"Close_t-7\"]].iloc[-1].values\n",
    "\n",
    "# === Calculate volatility: standard deviation as a simple proxy ===\n",
    "volatility_value = np.std(past_prices)\n",
    "\n",
    "print(f\"Volatility Estimate (std dev): {volatility_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Stock Recommendation: sell\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "recommendation_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Move the model to the selected device (GPU if available)\n",
    "recommendation_model = recommendation_model.to(device)\n",
    "\n",
    "# Map sentiment integer to readable string\n",
    "inv_sentiment_map = {v: k for k, v in sentiment_mapping.items()}\n",
    "sentiment_str = inv_sentiment_map.get(df[\"Sentiment\"].iloc[-1], \"neutral\")\n",
    "\n",
    "# Construct a more optimized and direct prompt for the model\n",
    "recommendation_input = (\n",
    "    f\"Given the stock trend '{trend}', confidence '{confidence}%', \"\n",
    "    f\"and sentiment '{sentiment_str}', provide a stock trading recommendation. \"\n",
    "    f\"Only respond with one of the following: 'buy', 'sell', or 'hold'. \"\n",
    "    f\"Make your recommendation based on the trend, sentiment, and confidence provided.\"\n",
    ")\n",
    "\n",
    "# Tokenize input and move input tensors to the selected device (GPU if available)\n",
    "input_ids = tokenizer(recommendation_input, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate recommendation\n",
    "output_ids = recommendation_model.generate(input_ids, max_new_tokens=20)\n",
    "recommendation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"ðŸ“ˆ Stock Recommendation: {recommendation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"stock\": \"AAPL\",\n",
      "    \"predicted_price\": \"$0.82\",\n",
      "    \"trend\": \"uptrend\",\n",
      "    \"confidence\": \"5.00%\",\n",
      "    \"recommendation\": \"sell\",\n",
      "    \"volatility_estimate\": \"6.9671\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Final Output as JSON ===\n",
    "import json\n",
    "\n",
    "result = {\n",
    "    \"stock\": df.iloc[-1][\"Ticker\"] if \"Ticker\" in df.columns else \"Invalid Ticker\",\n",
    "    \"predicted_price\": f\"${prediction_value:.2f}\",\n",
    "    \"trend\": trend,\n",
    "    \"confidence\": f\"{confidence}%\",\n",
    "    \"recommendation\": recommendation,\n",
    "    \"volatility_estimate\": f\"{volatility_value:.4f}\"\n",
    "}\n",
    "\n",
    "json_output = json.dumps(result, indent=4)\n",
    "print(json_output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
